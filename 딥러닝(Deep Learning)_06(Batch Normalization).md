# Batch Normalization

> 배치 정규화(Batch Normalization)은 딥러닝 네트워크 학습에서 Gradient Vanishing 또는 Exploding을 회피하기 위한 방법 중 하나로, 활성화 함수의 활성화 값 또는 출력 값을 정규화(정규분포로 만드는)하는 작업을 말한다.



![](https://charlesmartin14.files.wordpress.com/2017/06/batchnorm2-e1497643748774.png)

​		*노드들을 지나면서 값을 안정적으로 컨트롤 하고, 초기값에 대한 의존을 줄일 수 있다.*



### 배치 정규화 알고리즘

> 미니 배치(Mini - Batch)를 한 단위로 생각하고 정규화를 하는 것이며, 분포의 평균이 0 분산이 1 되도록 정규화 하는 과정이다.

- 하지만 층의 입력 값을 정규화 과정(평균 0, 분산 1)을 통해 고정 시키게되면 활성화 함수의 가장 중요한 역할인 비선형성을 없앨 수 있다는 위험이 있다.

- 이 점을 보완하기 위해 정규화된 값에 수학적 과정( **확대** scale 팩터(γ)와 **변환** shift 팩터(β))등을 도입해 활성화 함수로 들어가는 값의 범위를 바꾸어주면서 비선형성 가져갈 수 있도록 했다.

  ![](http://sanghyukchun.github.io/images/post/88-5.png)

  ![](C:\Users\bruce0809\Image\BN_image.jpg)

- 파란색 선이 배치 정규화후의 정확성 결과이며, 학습이 빠를 뿐 아니라 가중치 초기값의 영향을 받지 않는 것도 확인할 수있다.



### 특성 기억하기

1. *학습속도 개선*
2. *가중치 초기값 선택에 대한 의존성 낮아짐(학습때마다 출력을 정규화 하므로)*
3. *과적합의 위험을 줄일 수 있다.*
4. *기울기 값의 소멸 문제를 해결할 수 있다.*



##  참고한 자료

- 강사님 자료 (딥러닝 핵심자료 & 인공신경망 최적화 이론)
- 개인 홈페이지 (데이터 분석하는 문과생, 싸코 / https://sacko.tistory.com/)
- 책(케라스 창시자에게 배우는 딥러닝)