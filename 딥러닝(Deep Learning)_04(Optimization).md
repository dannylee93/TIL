# 딥러닝에서의 최적화(Optimization)

> 딥러닝에서 최적화(Optimization)는 손실함수(Loss Function)의 값을 최소화 하는 방향으로 파라미터 값을 찾는 것이다.



*딥러닝을 공부하면서 든 **개인적인 생각**은 딥러닝과 관련된  많은 기법들(예를들어  활성화 함수, 오차 역전파, 초기값 설정, 배치정규화, 드롭아웃 등)이 최적화(Optimization) 라는 큰 줄기 안에서 하위 카테고리로 있는 것 같다.* 

*딥러닝에서 뿐만이 아니라 크게 인공지능의 관점에서 봤을 때 **가장 중요한 단어가 **최적화가 아닐까 생각이 들었다.*

<img src="https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848" style="zoom:67%;" />

​				**위 사진처럼 다양한 최적화 기법을 찾아 하이퍼 파라미터의 조합을 찾아갈 수 있다.**

```shell
`변화 방향` : 경사하강법 >> 확률적 경사하강법 >> 모멘텀(방향중심) >> Adagard(학습률 중심) >> 합치기 시작한게 Adam
```

#### (1) 확률적 경사하강법(Stochastic Gradient Descent)

- 일부만 뽑아 보면서 속도를 높이는 방법이며,  일반 경사하강법과 같이 기울어진 방향으로 이동하는 방법이기 때문에 탐색 경로가 비효율적이라는 단점이 여전히 있다.
- 최적의 값을 찾아가는 방향이 뒤죽박죽이고, 보폭 정하기 어려운점이 있다.

#### (2) 모멘텀(Momentum)

- '운동량'의 느낌을 가지고 있다. 내려오는 속도가 크게 증가할수록 기울기 또한 크게 업데이트 되어 확률적 경사하강법의 단점을 보완할 수있다.

#### (3) Adagrad

- `학습률 감소` 방법처럼 학습을 진행하면서 학습률을 줄여나가는 방법이다.
- 과거의 기울기 값을 제곱해서 계속 더하는 식의 방법으로 학습률을 낮추는데 학습이 진행될수록 값이 0에 가까워 져서 학습의 정도가 크게 떨어진다.
- `RMS Prop` : 이전의 모든 기울기를 균일하게 더하지 않고 새로운 기울기 정보만 반영해서 학습률이 0으로까지 가까워 지는 것 방지

#### (4) Adam

- `Momentum` + `Adagrad` (momentum의 학습의 갱신 강도와 Adgrad의 학습률(보폭)을 줄여나가는 방법을 결합)

- 두 기법을 섞은 만큼 조정해야 될 하이퍼 파라미터 또한 많다.

  ![](https://t1.daumcdn.net/cfile/tistory/999A143359D86C022F)



### Mnist로 본 학습 최적화 그래프

![](https://t1.daumcdn.net/cfile/tistory/213D153857A0BA0D29)

> "ADAM: a method for stochastic optimization"라는 2015년에 발표된 논문자료 발췌
>
> 여러 상황을 고려했을 때 Adam을 많이 사용하고 있는 것 같다.



##  참고한 자료

- 강사님 자료 (딥러닝 핵심자료 & 인공신경망 최적화 이론)
- 개인 홈페이지 (데이터 분석하는 문과생, 싸코 / https://sacko.tistory.com/)
- 책(케라스 창시자에게 배우는 딥러닝)
- 논문("ADAM: a method for stochastic optimization")